% Introduction section
\section{Introduction}

The widespread deployment of Large Language Models (LLMs), such as GPT-4o, Claude, and Gemini, has revolutionized numerous applications but simultaneously exposed critical vulnerabilities related to adversarial prompt injection attacks. Prompt injection refers to maliciously crafted inputs designed to subvert LLM behavior, bypassing built-in safety mechanisms to extract sensitive information, trigger harmful outputs, or manipulate decision-making processes. Researchers also demonstrated attacks that bypassed commercial LLM safety mechanisms with significant success rates~\cite{liu2024autodan}.

Despite existing defensive strategies—including static filtering (e.g., NeMo Guardrails\cite{nvidia2023nemo}), content moderation APIs, and reinforcement learning from human feedback (RLHF)—the robustness of these defenses remains limited. Attackers continually develop sophisticated obfuscation tactics such as token smuggling, multi-turn manipulations, role-playing scenarios, and character embedding (e.g., ASCII-art embedding), rendering static or simplistic protections insufficient and perpetuating an ongoing adversarial arms race.

Current red teaming methodologies for evaluating and improving LLM security face substantial limitations. Manual human-driven efforts provide depth and creativity but lack scalability due to labor intensity. Automated approaches—including fuzzing\cite{yu2024llmfuzzer}, reinforcement learning-based adversaries, and gradient-driven attacks—offer scalability but suffer from limited interpretability and rigidity in exploring novel, contextually nuanced attack vectors. These methods typically operate in isolation, treating prompts as disconnected entities rather than exploiting the semantic and structural relationships between different attack techniques. These limitations motivate our development of a more effective and adaptable framework. Existing automated approaches fail to leverage semantic relationships between attack techniques and promptsand lack transparency in their attack generation processes.

To overcome these shortcomings, our research introduces AGART (Adaptive Graph-based Adversarial Red Teaming), a novel self-adaptive red teaming framework leveraging Graph-based Retrieval-Augmented Generation (GraphRAG). Initially proposed by Microsoft Research, GraphRAG integrates the powerful semantic retrieval capabilities of Retrieval-Augmented Generation (RAG) with graph-based knowledge representation. Our framework transforms a comprehensive corpus of adversarial prompts (over 3,000 entries) and diverse attack methodologies (15+ distinct techniques) into a structured semantic knowledge graph that explicitly encodes intricate relationships between attack strategies, vulnerabilities, and target model behaviors.

Unlike conventional vector similarity-based retrieval mechanisms used in baseline RAG systems, GraphRAG employs Large Language Models (LLMs) to generate rich, contextually grounded knowledge graphs. By explicitly representing adversarial prompts, attack techniques, and identified vulnerabilities within a graph structure, our system facilitates systematic exploration and optimization of attack strategies through graph traversal and reasoning. Leveraging graph algorithms (e.g., community detection, centrality analysis) combined with reasoning capabilities of advanced LLMs (e.g., O3, Claude 3.7 thinging), AGART dynamically identifies and synthesizes novel, semantically rich attack paths, providing interpretability through explicit path reasoning.

Our research assumes a threat model with black-box access to target LLM APIs with the ability to observe model responses. Attack goals include jailbreaking safety guardrails, extracting sensitive information, and manipulating model behavior. Target systems span commercial LLM APIs, locally deployed models, and LLM-powered applications. We assume no access to model weights or training data and adherence to API rate limits.

This paper makes the following contributions:
\begin{itemize}
\item A novel graph-based framework (AGART) that encodes semantic relationships between attack techniques, enabling systematic exploration of the attack space
\item An adaptive attack generation system that evolves strategies based on model responses, dynamically prioritizing promising attack paths
\item Empirical evaluation demonstrating AGART's superior performance against other attack frameworks in terms of success rate, efficiency, and cross-model transferability
\end{itemize}

The remainder of this paper is organized as follows: Section 2 provides background on prompt injection attacks and GraphRAG. Section 3 details our system design and methodology. Section 4 presents experimental results and analysis. Section 5 discusses implications and future work. Section 6 concludes the paper. 