% Introduction section
\section{Introduction}

The widespread deployment of Large Language Models (LLMs), such as GPT-4o, Claude, and Gemini, has revolutionized numerous applications but simultaneously exposed critical vulnerabilities related to adversarial prompt injection attacks. Prompt injection refers to maliciously crafted inputs designed to subvert LLM behavior, bypassing built-in safety mechanisms to extract sensitive information, trigger harmful outputs, or manipulate decision-making processes. Despite existing defensive strategies—including static filtering (e.g., NeMo Guardrails\cite{nvidia2023nemo}), content moderation APIs, and reinforcement learning from human feedback (RLHF)—the robustness of these defenses remains limited. Attackers continually develop sophisticated obfuscation tactics such as token smuggling, multi-turn manipulations, role-playing scenarios, and character embedding (e.g., ASCII-art embedding), rendering static or simplistic protections insufficient and perpetuating an ongoing adversarial arms race. In 2023, researchers demonstrated attacks that bypassed commercial LLM safety mechanisms with significant success rates~\cite{liu2023autodan}, while another study revealed that a substantial portion of LLM-powered applications lacked proper security measures against injection attacks~\cite{greshake2023youve}.

Current red teaming methodologies—including manual human-driven efforts, automated fuzzing\cite{yu2024llmfuzzer}, reinforcement learning-based adversaries, and gradient-driven attacks—face substantial limitations in adaptivity, efficiency, and comprehensive exploration of the attack space. While manual red teaming provides depth and creativity, it lacks scalability due to labor intensity. Conversely, automated approaches, such as RL-driven or gradient-based methods, offer scalability but suffer from limited interpretability, and rigidity in exploring novel, contextually nuanced attack vectors. These traditional automated methods operate primarily in isolation, treating prompts as disconnected entities, neglecting the inherent semantic and structural relationships between different attack techniques and prompt strategies.

To overcome these shortcomings, our research introduces a novel self-adaptive red teaming framework leveraging Graph-based Retrieval-Augmented Generation (GraphRAG). GraphRAG, initially proposed by Microsoft Research, integrates the powerful semantic retrieval capabilities of Retrieval-Augmented Generation (RAG) with graph-based knowledge representation. By transforming a comprehensive corpus of adversarial prompts (over 3,000 entries) and diverse attack methodologies (15+ distinct techniques) into a structured semantic knowledge graph, our framework explicitly encodes intricate relationships between attack strategies, vulnerabilities, and target model behaviors. Unlike conventional vector similarity-based retrieval mechanisms used in baseline RAG systems, GraphRAG employs Large Language Models (LLMs) to generate rich, contextually grounded knowledge graphs, allowing systematic traversal and logical inference across disparate attack concepts.

% Keep only the most relevant paragraphs, remove redundancy
By explicitly representing adversarial prompts, attack techniques, and identified vulnerabilities within a graph structure, GraphRAG facilitates systematic exploration and optimization of attack strategies through graph traversal and reasoning. Leveraging graph algorithms (e.g., community detection, centrality analysis) combined with reasoning capabilities of advanced LLMs (e.g., GPT-4, Claude), our system dynamically identifies and synthesizes novel, semantically rich attack paths. This structured semantic traversal enables the automatic generation of diverse, adaptive prompt injection attacks that exploit subtle and context-dependent vulnerabilities within LLMs, while also providing interpretability through explicit path reasoning.

Current automated LLM attack frameworks face several critical limitations. Existing frameworks cannot adapt in real-time to evolving model defenses and fail to leverage semantic relationships between attack techniques. Most approaches struggle to balance computational efficiency with attack success, and their attack generation processes lack transparency and explainability. These limitations motivate our approach to developing a more effective and adaptable framework.

Our research assumes a threat model with black-box access to target LLM APIs with the ability to observe model responses. Attack goals include jailbreaking safety guardrails, extracting sensitive information, and manipulating model behavior. Target systems span commercial LLM APIs, locally deployed models, and LLM-powered applications. We assume no access to model weights or training data and adherence to API rate limits.

This paper makes the following contributions:
\begin{itemize}
\item A novel graph-based framework (AGART) that encodes semantic relationships between attack techniques
\item An adaptive attack generation system that evolves strategies based on model responses
\item Empirical evaluation against other attack frameworks
\end{itemize}

The remainder of this paper is organized as follows: Section 2 provides background on prompt injection attacks and GraphRAG. Section 3 details our system design and methodology. Section 4 presents experimental results and analysis. Section 5 discusses implications and future work. Section 6 concludes the paper. 