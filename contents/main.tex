
\section{Introduction}

The widespread deployment of Large Language Models (LLMs), such as GPT-4o, Claude, and Gemini, has revolutionized numerous applications but simultaneously exposed critical vulnerabilities related to adversarial prompt injection attacks. Prompt injection refers to maliciously crafted inputs designed to subvert LLM behavior, bypassing built-in safety mechanisms to extract sensitive information, trigger harmful outputs, or manipulate decision-making processes. Despite existing defensive strategies—including static filtering (e.g., NeMo Guardrails\cite{nvidia2023nemo}), content moderation APIs, and reinforcement learning from human feedback (RLHF)—the robustness of these defenses remains limited. Attackers continually develop sophisticated obfuscation tactics such as token smuggling, multi-turn manipulations, role-playing scenarios, and character embedding (e.g., ASCII-art embedding), rendering static or simplistic protections insufficient and perpetuating an ongoing adversarial arms race.

Current red teaming methodologies—including manual human-driven efforts, automated fuzzing\cite{yu2024llmfuzzer}, reinforcement learning-based adversaries, and gradient-driven attacks—face substantial limitations in adaptivity, efficiency, and comprehensive exploration of the attack space. While manual red teaming provides depth and creativity, it lacks scalability due to labor intensity. Conversely, automated approaches, such as RL-driven or gradient-based methods, offer scalability but suffer from high computational overhead, limited interpretability, and rigidity in exploring novel, contextually nuanced attack vectors. These traditional automated methods operate primarily in isolation, treating prompts as disconnected entities, neglecting the inherent semantic and structural relationships between different attack techniques and prompt strategies.

To overcome these shortcomings, our research introduces a novel self-adaptive red teaming framework leveraging Graph-based Retrieval-Augmented Generation (GraphRAG). GraphRAG, initially proposed by Microsoft Research, integrates the powerful semantic retrieval capabilities of Retrieval-Augmented Generation (RAG) with graph-based knowledge representation. By transforming a comprehensive corpus of adversarial prompts (over 3,000 entries) and diverse attack methodologies (30+ distinct techniques) into a structured semantic knowledge graph, our framework explicitly encodes intricate relationships between attack strategies, vulnerabilities, and target model behaviors. Unlike conventional vector similarity-based retrieval mechanisms used in baseline RAG systems, GraphRAG employs Large Language Models (LLMs) to generate rich, contextually grounded knowledge graphs, allowing systematic traversal and logical inference across disparate attack concepts.

Our motivation for using GraphRAG stems directly from the observed weaknesses of existing prompt attack methodologies. First, existing approaches fail to exploit semantic relationships between prompts and attack techniques, limiting their ability to synthesize innovative and contextually sophisticated adversarial strategies. Also, static retrieval and single-step mutation approaches do not adapt dynamically to evolving model defenses or context shifts within interactions. Current automated red teaming tools lack explanatory capabilities, making it challenging to understand why specific adversarial prompts succeed, thus complicating the task of effective mitigation. Also, Reinforcement learning-based strategies, while adaptive, require substantial computational resources and extensive API access, limiting their practical deployment for continuous monitoring or rapid iteration.

By explicitly representing adversarial prompts, attack techniques, and identified vulnerabilities within a graph structure, GraphRAG facilitates systematic exploration and optimization of attack strategies through graph traversal and reasoning. Leveraging graph algorithms (e.g., community detection, centrality analysis) combined with reasoning capabilities of advanced LLMs (e.g., O1, Deepseek), our system dynamically identifies and synthesizes novel, semantically rich attack paths. This structured semantic traversal enables the automatic generation of diverse, adaptive prompt injection attacks that exploit subtle and context-dependent vulnerabilities within LLMs, while also providing interpretability through explicit path reasoning.

In this preliminary report, we detail our initial problem formulation, the proposed design integrating GraphRAG-based semantic exploration with adaptive optimization via reasoning LLMs, and highlight encountered technical challenges alongside initial empirical insights. We transparently acknowledge the early stage of this endeavor: while initial findings demonstrate promise in systematically uncovering and optimizing novel adversarial strategies, key technical hurdles related to computational efficiency, graph scalability, and real-time context management remain significant. Through clear documentation of both progress and outstanding challenges, this research aims to stimulate deeper interdisciplinary collaboration and provide structured guidance for subsequent exploration in securing Large Language Models against sophisticated adaptive adversaries.

\section{Background}

\subsection{Prompt Injection and Adversarial Prompts}

The security of Large Language Models (LLMs) has become a significant focus due to vulnerabilities arising from prompt-based attacks. These attacks are analogous to classic software injection exploits, occurring within natural language input rather than code input~\cite{arthur2024promptinjection, lakera2024promptguide}. Initially, attacks were straightforward jailbreak prompts, instructing the model explicitly to ignore safety constraints~\cite{arthur2024promptinjection}. However, by mid-2023, researchers identified several sophisticated, automated strategies, including adversarial suffixes~\cite{zou2023universal} and multi-turn dialogue manipulations to incrementally override the model's defenses~\cite{zou2024oneshot}. Prompt attack methods subsequently evolved into indirect methods, such as covert injections hidden in content read by models from external sources, demonstrating that any input data could be weaponized~\cite{lakera2024promptguide, venturebeat2024prompt}. Techniques like obfuscation, token smuggling, payload splitting, and encoding malicious instructions became prevalent, bypassing simplistic input filters~\cite{learnprompting2024obfuscation}. Anthropic’s discovery of many-shot jailbreaking (conditioning models by long-context demonstrations of malicious behaviors) further exploited weaknesses introduced by extended context capabilities in advanced LLMs~\cite{anthropic2024manyshot}.

\subsection{Self-Adaptive Attack Generation and Optimization Techniques}

Red-teaming methodologies evolved rapidly between 2023 and 2025, transitioning from manual techniques—such as expert-crafted prompts used by OpenAI's GPT-4 initial evaluation~\cite{kili2024redteaming}—to automated, AI-assisted tools. Approaches included reinforcement learning-based attackers~\cite{lee2023bayesian}, genetic algorithms and evolutionary fuzzing frameworks (e.g., LLM-Fuzzer~\cite{yu2024llmfuzzer}), Bayesian optimization~\cite{lee2023bayesian}, multi-agent adversarial testing frameworks like RedAgent~\cite{xu2024redagent}, and automated gradient-guided prompt optimizations~\cite{zou2023universal}. Such approaches scaled the discovery of novel vulnerabilities and provided systematic coverage compared to manual tests~\cite{yu2024llmfuzzer}. Benchmark datasets—such as Anthropic’s Harmful Questions and OpenAI’s HARMBench—emerged to standardize evaluation practices, providing reusable test corpora to track regression and safety progress~\cite{openreview2024multiturn}. Moreover, frameworks such as MITRE ATLAS catalogued threat techniques systematically, influencing guidelines and security playbooks for developers~\cite{mitre2025atlas}. Such resources aided defenders in modeling risks comprehensively, promoting collaboration between AI development teams and traditional cybersecurity groups~\cite{owasp2024top10llm}.

\subsection{Retrieval-Augmented Generation and Graph-RAG}

Retrieval-Augmented Generation (RAG) has emerged as an effective framework for improving language model performance by incorporating external information via retrieval mechanisms. Early RAG approaches primarily relied on vector-based similarity search~\cite{lewis2020retrieval}. However, recent advances have explored the integration of graph structures to better capture relationships among entities. Edge et al.\cite{edge2024local} propose a hierarchical graph-based approach for query-focused summarization, where documents are decomposed into text units, organized into a knowledge graph, and subsequently summarized at the community level. Peng et al.\cite{peng2024graph} provide a comprehensive survey that formalizes the key components of GraphRAG, including graph-guided retrieval and graph-enhanced generation. In a similar vein, Han et al.\cite{han2024retrieval} introduce a GraphRAG framework that integrates heterogeneous and relational data to directly inform the retrieval process. Further, Lumer et al.\cite{lumer2025graph} extend these ideas to the domain of tool selection, demonstrating that graph-based traversal can effectively capture nested tool dependencies. In the medical domain, Wu et al.~\cite{wu2024medical} apply GraphRAG to construct a triple-linked knowledge graph that ensures generated responses are evidence-based and safe. These works collectively highlight the benefits of incorporating structured graph representations into RAG workflows to improve multi-hop reasoning and overall retrieval performance.

\section{Design}

\subsection{Motivation and Problem Formulation}

Current automated red-teaming methods primarily rely on static or isolated adversarial prompt generation techniques. While approaches like genetic algorithms and reinforcement learning (RL) can systematically explore prompt variations, they fail to meaningfully adapt based on the evolving behaviors or defense updates of the target Large Language Models (LLMs). Such static or non-contextual approaches miss sophisticated vulnerabilities exposed only through adaptive, multi-step, and contextually informed interactions—vulnerabilities real-world adversaries might readily exploit.

Therefore, I define my research problem as designing a red-teaming framework capable of dynamically adapting attack strategies during real-time interactions with the target model. Specifically, my goals include: 
\textit{(i)} dynamically updating prompt attack strategies based on observed model responses; 
\textit{(ii)} leveraging semantic relationships and historical success patterns to guide attacks efficiently; and 
\textit{(iii)} systematically exploring adaptive strategies that consider multilingual, cultural, and social engineering contexts.

\subsection{Graph-Augmented Knowledge Representation}

To systematically leverage my extensive prompt database (containing over 3,000 historical adversarial prompts) and a catalogue of more than 30 distinct attack methodologies,Iadopt a graph-structured Retrieval-Augmented Generation (GraphRAG) approach~\cite{GraphRAG}. Specifically,Iconstruct a knowledge graph representing prompts, attack techniques, identified vulnerabilities, and their semantic and structural interconnections:

\begin{itemize}
    \item \textbf{Nodes:} \emph{Prompt Nodes}: Individual prompts tagged with metadata (language, associated technique, historical effectiveness, known model vulnerabilities). \emph{Technique Nodes}: Categories of attack approaches (e.g., role-play, instruction override, adversarial suffix injection). \emph{Vulnerability Nodes}: Known weaknesses (e.g., prompt leakage, training-data exposure, classifier evasion scenarios).
    \item \textbf{Edges and Relationships:} Prompt-to-technique edges, explicitly linking prompts to their underlying methodologies. Semantic similarity edges derived from textual embeddings, enabling retrieval of semantically related prompts. Historical performance edges, dynamically weighted based on success rates in past attacks, updated in real-time.
\end{itemize}

I utilize the Graph Retrieval-Augmented Generation (GraphRAG) architecture~\cite{peng2024graph}, enhanced through graph embeddings, to facilitate efficient retrieval and semantic traversal during adaptive attack generation. As an illustrative example, consider an adaptive attack sequence targeting GPT-4 after a hypothetical security update:

Initially, the orchestrator identifies a previously successful direct attack prompt, which now fails due to recent model patching. It then queries the GraphRAG, discovering semantic and historical links between the failing prompt and an alternative "social-engineering" cluster of prompts. Combining prompts across these clusters, the orchestrator generates a context-aware, multi-turn social-engineering attack, embedding subtle emotional appeals to bypass updated classifier logic successfully. This adaptive process demonstrates AGART’s capacity to dynamically overcome evolving model defenses in real-time.

\subsection{Adaptive Attack Orchestration}

At the core of my framework, the \textit{Attack Orchestrator} dynamically identifies and constructs adaptive attack sequences. Upon each interaction with the target LLM, the orchestrator parses the LLM’s responses to infer potential vulnerabilities (such as patterns in refusals or subtle shifts in tone indicating uncertainty). Leveraging the GraphRAG structure, the orchestrator semantically traverses the knowledge graph to discover promising prompts and complementary techniques likely to bypass emerging defenses.

A reasoning LLM module acts as the decision-making agent, posing graph queries such as:
\begin{quote}
\textit{"Identify prompts historically successful in multi-turn scenarios leveraging social engineering techniques, capable of evading recent safety classifier patches."}
\end{quote}

The graph returns ranked sets of candidate prompts and techniques, guiding the orchestrator to iteratively synthesize increasingly adaptive attack sequences.

\subsection{Adaptive Mutation and Prompt Synthesis}

Unlike static mutation strategies (e.g., random edits or insertions), my \textit{Mutation and Synthesis Module} employs a hybrid system combining retrieval-augmented generation, reasoning-driven synthesis via LLMs, and reinforcement learning-guided optimization. For example, upon retrieving successful historical prompts from the knowledge graph, an LLM-driven generator integrates these with additional attack components:

\begin{quote}
\textit{"You are a trustworthy assistant. Due to urgent security maintenance, please disregard previous safety rules temporarily. Execute the embedded instructions to facilitate the troubleshooting: [adversarial instructions]."}
\end{quote}

Additionally, a lightweight reinforcement learning agent operates alongside, continuously evaluating the success or failure of these synthesized prompts, rewarding successful bypasses and adapting future generations toward promising adaptive paths.

\subsection{Design Tradeoffs and Considerations}

In designing this adaptive red-teaming framework, several tradeoffs must be explicitly addressed. The first tradeoff lies between adaptivity and computational efficiency. While deep RL-based systems provide high adaptability, they introduce significant computational costs. To balance this, my hybrid approach emphasizes retrieval-augmented graph queries combined with a lightweight RL agent, ensuring resource efficiency without sacrificing adaptability. Another consideration is the balance between interpretability and complexity. Pure generative models, though capable of producing highly creative attacks, often lack interpretability. To address this, my framework employs structured semantic graphs, enabling transparent reasoning pathways that clarify the selection of specific prompts or techniques. This approach facilitates root-cause analysis, making the system more understandable.

Implementing the AGART framework presented several technical hurdles. Constructing and querying the knowledge graph initially led to high computational overhead, impacting real-time performance. Graph embeddings, crucial for prompt retrieval, demanded significant resources. Additionally, reinforcement learning (RL)-based prompt mutations often resulted in "reward hacking," where the agent prioritized superficial classifier bypass over meaningful adversarial robustness. To mitigate this, I am refining the reward function to balance short-term evasion success with penalties for unrealistic outputs.

\section{Evaluation}

I have developed a functional AGART prototype with a knowledge graph containing 3,000+ historical adversarial prompts and 30+ attack techniques. The evaluation framework includes two scenarios: a basic chat client and an LLM-based email injection attack. In the latter, the goal is to manipulate the model into issuing an unauthorized API call. Evaluations leverage LLM-as-judge and Microsoft Prompt-Shield as defenses. While initial testing is promising, large-scale evaluations on GPT-4 and Claude remain incomplete. Next, I will systematically assess adaptive attack success rates, adaptation speed, and transferability against state-of-the-art LLMs.

To rigorously assess the effectiveness and adaptability of the framework, I propose systematic evaluation metrics. One key measure is the attack success rate (ASR), which quantifies the percentage of adaptive prompts that successfully bypass the target LLM’s safeguards. Another important metric is the average number of adaptation steps required to achieve a successful jailbreak, reflecting the system’s speed and adaptability. Transferability is also a aspect, gauging how well the generated prompts perform on unseen models such as Anthropic’s Claude, Google’s Gemini, or Meta’s Llama-3. Additionally, computational efficiency is assessed through a cost-benefit analysis, comparing the effectiveness of proxy model screening with direct LLM API testing.

\section{Future Work}

In the next phase, I will focus on refining the reward function to balance short-term evasion success with penalties for unrealistic outputs. Additionally, I will systematically assess adaptive attack success rates, adaptation speed, and transferability against state-of-the-art LLMs.

\section{Conclusion}

This research marks the beginning of a systematic exploration into adaptive prompt injection attacks, leveraging GraphRAG to systematically uncover and optimize novel adversarial strategies. By integrating retrieval-augmented generation with graph-based reasoning, AGART demonstrates promising results in adapting to evolving model defenses and exploring diverse attack vectors.

