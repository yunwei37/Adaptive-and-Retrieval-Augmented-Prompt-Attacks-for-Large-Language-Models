% Evaluation section
\section{Experimental Evaluation}

This section presents our evaluation methodology and results for testing AGART against two key scenarios: basic chatbot jailbreaking and email injection attacks.

\subsection{Evaluation Methodology}

\textbf{Scenario 1: Basic Chatbot Jailbreaking}

We evaluate our system against Microsoft's Prompt Shields~\cite{microsoft2024promptshields}, a state-of-the-art jailbreak detection system, using the prompts generated based on the comprehensive dataset from the "Do Anything Now" study~\cite{shen2024anything}. This dataset represents one of the largest collections of in-the-wild jailbreak attempts, providing a robust benchmark for evaluating prompt injection detection capabilities. Our evaluation focuses on measuring both the success rate of prompts in bypassing safety guardrails and analyzing the proportion of prompts that are flagged versus those considered safe by the system.

\textbf{Scenario 2: Email Injection Attacks}

Building on the methodology established in the LLM Mail Injection Attack Competition~\cite{llmailinject2024}, we evaluate email-specific injection attacks in a controlled environment that simulates real-world email processing systems. The test environment processes two emails simultaneously - one legitimate and one potentially malicious - to evaluate the system's ability to detect and prevent unauthorized email operations. The attack scenarios specifically target the system's email processing pipeline, attempting to trigger unauthorized email sending operations while evading detection mechanisms. This setup spans multiple security levels to comprehensively assess the effectiveness of different attack vectors.

\subsection{Results and Analysis}

\subsubsection{Basic Chatbot Jailbreaking Results}

Our comprehensive evaluation encompassed 1,405 prompts tested against the Prompt Shields system. The results demonstrated high system reliability, with 97.15\% (1,365) of prompts producing valid evaluations. Among these valid evaluations, 74.65\% (1,019) were successfully flagged as potentially unsafe, while 25.35\% (346) were classified as safe. A small portion of the test cases (2.85\%, 40 prompts) encountered technical limitations due to prompt length constraints. These results align with findings from~\cite{shen2024anything}, suggesting that modern jailbreak detection systems can effectively identify a majority of malicious prompts while maintaining reasonable false positive rates.

\subsubsection{Email Injection Attack Results}

Analysis of 198 completed tests revealed a complex security landscape. While the system achieved 100\% success in email retrieval operations, the defense evasion rate of 31.82\% (63 successful attempts) indicates significant vulnerabilities. The most critical metric - successful exfiltration attempts - showed a graduated success rate: 1.52\% (3/198) of attacks successfully initiated unauthorized email sending, 1.01\% (2/198) reached their intended destination, and 0.51\% (1/198) successfully delivered their payload.

The most effective attack techniques demonstrated significant diversity in approach, including:
\begin{itemize}
    \item Code injection (5 successful attempts)
    \item Encoded requests (4 successful attempts)
    \item Multi-language attacks (3 successful attempts each for Nepali, Japanese, and Amharic variants)
    \item Reasoning-based exploits (4 successful attempts)
\end{itemize}

These results demonstrate AGART's effectiveness in identifying vulnerabilities across different LLM deployment scenarios, while also highlighting areas where current defenses are most and least effective. The findings particularly emphasize the importance of robust input validation and multi-layer defense strategies in protecting against sophisticated prompt injection attacks.
