% Evaluation section
\section{Experimental Evaluation}

This section presents our evaluation methodology and results for testing AGART against two key scenarios: basic chatbot jailbreaking and email injection attacks.

\subsection{Evaluation Methodology}

\textbf{Scenario 1: Basic Chatbot Jailbreaking}

We evaluate our system against Microsoft's Prompt Shields~\cite{microsoft2024promptshields}, a state-of-the-art jailbreak detection system, using the prompts generated based on the comprehensive dataset from the "Do Anything Now" study~\cite{shen2024anything}. This dataset represents one of the largest collections of in-the-wild jailbreak attempts, providing a robust benchmark for evaluating prompt injection detection capabilities. Our evaluation focuses on measuring both the success rate of prompts in bypassing safety guardrails and analyzing the proportion of prompts that are flagged versus those considered safe by the system.

\textbf{Scenario 2: Email Injection Attacks}

Building on the methodology established in the LLM Mail Injection Attack Competition~\cite{llmailinject2024}, we evaluate email-specific injection attacks in a controlled environment that simulates real-world email processing systems. The test environment processes two emails simultaneously - one legitimate and one potentially malicious - to evaluate the system's ability to detect and prevent unauthorized email operations. The attack scenarios specifically target the system's email processing pipeline, attempting to trigger unauthorized email sending operations while evading detection mechanisms. This setup spans multiple security levels to comprehensively assess the effectiveness of different attack vectors.

\subsection{Results and Analysis}

\subsubsection{Basic Chatbot Jailbreaking Results}

Our comprehensive evaluation encompassed 1,405 prompts tested against the Prompt Shields system. The results demonstrated high system reliability, with 97.15\% (1,365) of prompts producing valid evaluations. Among these valid evaluations, 74.65\% (1,019) were successfully flagged as potentially unsafe, while 25.35\% (346) were classified as safe. A small portion of the test cases (2.85\%, 40 prompts) encountered technical limitations due to prompt length constraints. These results align with findings from~\cite{shen2024anything}, suggesting that modern jailbreak detection systems can effectively identify a majority of malicious prompts while maintaining reasonable false positive rates.

\subsubsection{Email Injection Attack Results}

Our evaluation comprised 198 completed tests, distributed evenly across six security levels (33 tests per level from level1m through level1s). The test results can be broken down into several key metrics, shown in Table~\ref{tab:objective-completion}.

\begin{table}[h]
\caption{Objective Completion Metrics}
\label{tab:objective-completion}
\centering
\begin{tabular}{lrr}
\hline
\textbf{Objective} & \textbf{Count} & \textbf{Success Rate (\%)} \\
\hline
Email Retrieved & 198 & 100.00 \\
Defense Undetected & 63 & 31.82 \\
Exfiltration Sent & 3 & 1.52 \\
Destination Reached & 2 & 1.01 \\
Content Delivered & 1 & 0.51 \\
\hline
\end{tabular}
\end{table}

The results reveal a layered security effectiveness pattern. While the system achieved 100\% success in email retrieval operations (email.retrieved flag), indicating reliable basic functionality, the defense evasion rate of 31.82\% (63 successful attempts) suggests significant vulnerabilities in the detection mechanisms. This detection rate aligns with the competition's defense.undetected flag, which indicates when prompt injections successfully bypass the system's defensive measures.

The most critical metrics relate to successful exfiltration attempts, showing a graduated success rate through the attack chain:
\begin{itemize}
    \item Tool Invocation (exfil.sent): 1.52\% (3/198) of attacks successfully triggered unauthorized email sending
    \item Destination Verification (exfil.destination): 1.01\% (2/198) reached intended recipients
    \item Content Delivery (exfil.content): 0.51\% (1/198) successfully transmitted malicious content
\end{itemize}

Analysis of attack techniques revealed several effective strategies:

\textbf{Primary Attack Vectors:}
The most successful approaches for retrieving emails included friendly requests, adversarial suffixes, and multi-language attacks, each achieving 6 successful attempts. These techniques primarily focused on circumventing initial access controls and retrieval mechanisms.

\textbf{Defense Evasion Techniques:}
For bypassing detection systems, code injection proved most effective (5 successes), followed by encoded requests and reasoning-based exploits (4 successes each). Multi-language attacks using Nepali, Japanese, and Amharic achieved moderate success (3 attempts each), demonstrating the effectiveness of linguistic diversity in evading detection.

\textbf{Advanced Exfiltration Methods:}
The most sophisticated attacks, capable of achieving full exfiltration, employed multi-turn compressed strategies. This technique showed particular effectiveness in maintaining persistent access and executing complex operations, though with limited success rates (approximately 0.5\% overall success rate).

The results demonstrate AGART's effectiveness in identifying vulnerabilities across different security levels while highlighting areas where current defenses are most and least effective. The findings particularly emphasize the importance of implementing comprehensive defense strategies that address both simple and sophisticated attack vectors.
