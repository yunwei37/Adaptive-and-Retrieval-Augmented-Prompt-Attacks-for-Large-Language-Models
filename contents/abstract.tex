\begin{abstract}
    Large Language Models (LLMs) have demonstrated vulnerability to prompt injection attacks despite widespread defensive measures. This paper introduces AGART (Adaptive Graph-based Adversarial Red Teaming), a novel framework that dynamically generates and adapts adversarial prompts using a structured graph representation of attack techniques. Unlike previous approaches that treat prompts as isolated entities, AGART encodes semantic relationships between attack strategies, enabling systematic exploration of the attack space. The framework integrates Graph-based Retrieval-Augmented Generation (GraphRAG) to effectively traverse and reason about attack patterns, dynamically adapting strategies based on model responses. Our experimental demonstrates that AGART significantly outperforms gradient-based and genetic algorithm approaches in terms of attack success rates while requiring fewer adaptation steps. Additionally, AGART-generated attacks show higher transferability across models compared to baselines. Through ablation studies, we validate the importance of graph structure and adaptation mechanisms. We conclude with recommendations for robust LLM deployment and discuss responsible disclosure practices.
\end{abstract}

% Our experimental evaluation across four state-of-the-art LLMs demonstrates that AGART achieves 67-85\% attack success rates, outperforming gradient-based and genetic algorithm approaches by 15-25\% while requiring 40\% fewer adaptation steps. Additionally, AGART-generated attacks show higher transferability across models (43-72\%) compared to baselines. Through ablation studies, we validate the importance of graph structure and adaptation mechanisms. We conclude with recommendations for robust LLM deployment and discuss responsible disclosure practices.

% Our key contributions include a graph-based framework for adaptive prompt injection attacks against state-of-the-art LLM defenses; a comprehensive evaluation across commercial and open-source LLMs demonstrating improved transferability of generated attacks; an open-source implementation and dataset of adversarial prompts for reproducible research; and ethical considerations with responsible disclosure guidelines for secure LLM deployment.

% \textbf{Keywords:} Large Language Models, Prompt Injection, Graph-RAG, Adaptive Attacks, Red Teaming, AI Security
