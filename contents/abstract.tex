\begin{abstract}
    Recent advances in Large Language Models (LLMs) have revealed severe vulnerabilities to adversarial manipulations through carefully crafted prompt injections. While numerous defense techniques—ranging from rule-based filters and regexes to sophisticated alignment strategies—have been proposed, these remain inherently brittle, easily bypassed by adaptive attackers employing obfuscation techniques such as token smuggling, multi-turn injections, or encoding-based attacks. This paper introduces a research initiative aimed at developing self-adaptive prompt injection attacks that dynamically leverage Retrieval-Augmented Generation (RAG) combined with structured graph-based exploration techniques. my proposed approach continuously updates its adversarial strategies by retrieving external attack patterns and historical adversarial examples, systematically exploring a diverse and high-dimensional prompt-space via a graph traversal mechanism. While the project is ongoing, this report details my initial findings, the conceptual design, encountered technical challenges, and preliminary results. I highlight critical trade-offs, including the balance between robustness of the attack mechanism and its computational scalability. my work-in-progress sets a clear research agenda towards more resilient, automated, and adaptive adversarial prompt attacks, anticipating future contributions to rigorous security testing methodologies and ultimately improved model defenses.
    \end{abstract}
    