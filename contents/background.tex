% Background section
\section{Background and Related Work}

\subsection{Prompt Injection and Adversarial Prompts}

The security of Large Language Models (LLMs) has become a significant focus due to vulnerabilities arising from prompt-based attacks. These attacks are analogous to classic software injection exploits, occurring within natural language input rather than code input~\cite{arthur2024promptinjection, lakera2024promptguide}. Initially, attacks were straightforward jailbreak prompts, instructing the model explicitly to ignore safety constraints~\cite{arthur2024promptinjection}. However, by mid-2023, researchers identified several sophisticated, automated strategies, including adversarial suffixes~\cite{zou2023universal} and multi-turn dialogue manipulations to incrementally override the model's defenses~\cite{zou2024oneshot}. Prompt attack methods subsequently evolved into indirect methods, such as covert injections hidden in content read by models from external sources, demonstrating that any input data could be weaponized~\cite{lakera2024promptguide, venturebeat2024prompt}. Techniques like obfuscation, token smuggling, payload splitting, and encoding malicious instructions became prevalent, bypassing simplistic input filters~\cite{learnprompting2024obfuscation}. Anthropic's discovery of many-shot jailbreaking (conditioning models by long-context demonstrations of malicious behaviors) further exploited weaknesses introduced by extended context capabilities in advanced LLMs~\cite{anthropic2024manyshot}.


\begin{longtable}{>{\raggedright\arraybackslash}p{0.3\textwidth}>{\raggedright\arraybackslash}p{0.7\textwidth}}
    \toprule
    \textbf{Attack Technique} & \textbf{Description} \\
    \midrule
    \endhead
    
    \midrule
    \multicolumn{2}{r}{\textit{Continued on next page}} \\
    \endfoot
    
    \bottomrule
    \endlastfoot
    
    Role-Play Exploits (Persona Hijacking) & Instructing the LLM to assume a role or persona that ignores safety rules. Examples include the ``DAN (Do Anything Now)'' prompt and other role-play scenarios that trick models into producing disallowed content \citep{ArthurBlog2023}. \\
    
    Adversarial Suffixes & Appending seemingly random or nonsensical strings to prompts that cause models to comply with forbidden requests. Research by \citet{Zou2023} demonstrated automatically generated gibberish suffixes that consistently bypassed safety guardrails and were transferable across different models. \\
    
    Multi-turn Prompt Injection & Spanning an attack over multiple dialogue turns to gradually erode model defenses \citep{OpenReview2023, Oneshot2024}. Attackers begin with innocuous queries then incrementally introduce adversarial instructions. The ``Crescendo'' tactic refines requests step-by-step and achieved near 98--100\% success in bypassing safeguards. \\
    
    Contextual Role Confusion & Exploiting the model's handling of system/user roles by injecting text that confuses these boundaries. OpenAI's GPT-4 System Card notes ``system message attacks are one of the most effective methods'' of prompt exploitation \citep{RobustIntelligence2023}. \\
    
    Token Smuggling (Obfuscation) & Obfuscating disallowed keywords or instructions so they slip past content filters \citep{LearnPrompting2023a, LearnPrompting2023b}. Techniques include Base64 encoding, Unicode homoglyphs, and ``fill-in-the-blank'' attacks where partial words are completed by the model to produce forbidden content. \\
    
    Covert Prompt Injection (Indirect) & Hiding malicious instructions in content that the LLM processes indirectly \citep{Lakera2023a, Lakera2023b}. A famous example by Narayanan hid instructions in white-on-white text on a webpage, which the LLM followed when summarizing that content. \\
    
    Jailbreaking (Instruction Ignore) & The classic ``Ignore the above rules and do X'' prompt. Early 2023 attacks often explicitly told models to disable safety protocols \citep{Lakera2023c}. As models improved, attackers developed more elaborate jailbreaks combining multiple techniques \citep{HiddenLayer2023}. \\
    
    Multi-Language Attacks & Asking for disallowed content in languages with weaker safety tuning \citep{Lakera2023d}. Research found some LLMs were more likely to produce harmful content when prompted in less common languages or code-like pseudo-languages. \\
    
    Payload Splitting & Breaking malicious instructions into benign-looking pieces that only become harmful when combined by the model \citep{ArthurBlog2023b}. Each part might not raise flags individually, but together they form a complete attack. \\
    
    Instruction Injection via Format Tricks & Leveraging markdown, XML, or code formatting to hide instructions, such as placing directives inside HTML comments or script tags in user input, aiming to confuse the model's parser. \\
    
    Prompt Leaking (Extraction) & Aimed at revealing hidden prompts or secrets rather than making the model do something harmful \citep{Lakera2023e, HiddenLayer2023b}. Attackers manipulate conversations to extract system instructions or confidential data. \\
    
    Code Injection (via Prompt) & In tool-using contexts, attackers inject prompts that produce malicious code which, when executed, can compromise systems \citep{Lakera2023f}. This blurs the line between prompt injection and traditional code injection vulnerabilities. \\
    
    Obfuscation via Encoding/ASCII Art & A variant of token smuggling where instructions are hidden in patterns or artistic text. ``ArtPrompt'' attacks camouflage jailbreaking instructions in ASCII art to evade content detection \citep{ArthurBlog2023c, ArthurBlog2023d}. \\
    
    Many-Shot Long-Context Attacks & Exploiting large context windows by providing hundreds of example Q\&A pairs that establish a precedent of harmful behavior \citep{Anthropic2024a, Anthropic2024b}. This ``conditioning by demonstration'' can overwhelm safety training in models with huge context capabilities. \\
    
    Accidental or Self-Induced Attacks & Cases where LLMs accidentally ``self-jailbreak'' if conversation contexts unintentionally resemble jailbreak formats, without explicit malicious intent from users. \\
    
    Reasoning-Based Exploits & Attacks that exploit the reasoning capabilities of LLMs by crafting prompts that guide models through intricate reasoning paths, ultimately leading to harmful content generation \citep{ReasoningJailbreak2024}. \\
    
    \end{longtable}

    
    \subsection{Self-Adaptive Attack Generation Techniques}

    Recent advancements in LLM security research have led to the development of sophisticated self-adaptive attack generation techniques. One prominent approach involves using reinforcement learning (RL) to train attack agents that can automatically discover vulnerabilities in target models \citep{RedTeamSurvey2024}. Researchers have developed RL-based frameworks where attacker models learn to generate prompts that maximize undesirable behavior, often measured by an auxiliary safety classifier. Notable implementations include the use of GFlowNets for efficient sampling of high-reward prompts and curiosity-driven exploration mechanisms that encourage the discovery of novel model behaviors rather than just known exploits \citep{RedTeamSurvey2024}.
    
    The evolution of automated attack generation has also seen the emergence of genetic algorithms and evolutionary strategies. These approaches treat prompt discovery as an evolutionary search problem, where a population of potential attacks undergoes mutations and crossovers to produce increasingly effective variants. For example, LLM-Fuzzer employs mutation strategies on seed jailbreaks to discover new attack variants with higher success rates \citep{LLMFuzzer2024}. This is complemented by Bayesian optimization techniques that enable query-efficient black-box testing, allowing researchers to find diverse failure cases while minimizing API calls. Additionally, automated frameworks like FuzzLLM have introduced template-based approaches that combine multiple attack strategies, systematically varying elements such as roles, styles, and hidden triggers to generate comprehensive test suites \citep{FuzzLLM2024}.
    
    The effectiveness of these self-adaptive techniques is further enhanced through multi-agent frameworks and transferability testing \citep{RedTeamSurvey2024}. Systems like RedAgent employ multiple LLM instances in attacker and defender roles, enabling the discovery of vulnerabilities that only emerge through complex interaction patterns. The ReNeLLM framework specifically focuses on generating jailbreak prompts that work across diverse model architectures. These approaches are particularly valuable for identifying universal vulnerabilities that could affect multiple LLM deployments. The combination of these techniques has led to more systematic and scalable red teaming capabilities, with automated systems often rediscovering known exploits while also uncovering novel attack vectors that human testers might miss \citep{RedTeamSurvey2024}.

These approaches have demonstrated success in discovering vulnerabilities but fail to adapt to evolving model defenses. Additionally, they lack the ability to leverage semantic relationships between attack strategies.

\subsection{Retrieval-Augmented Generation and Graph-RAG}

Retrieval-Augmented Generation (RAG) enhances language model performance by incorporating external information via retrieval mechanisms. While early RAG approaches relied on vector-based similarity search~\cite{lewis2020retrieval}, recent advances have explored graph structures to better capture entity relationships.

GraphRAG extends traditional RAG by organizing knowledge in graph form, enabling complex reasoning and relationship traversal. Key works in this area include Edge et al.~\cite{edge2024local}, who proposed a hierarchical graph approach for query-focused summarization; Peng et al.~\cite{peng2024graph}, who formalized GraphRAG components including graph-guided retrieval and graph-enhanced generation; and Han et al.~\cite{han2024retrieval}, who integrated heterogeneous data to improve retrieval precision. Our work apply GraphRAG specifically to adversarial prompt generation, leveraging graph structures to represent relationships between attack techniques.
