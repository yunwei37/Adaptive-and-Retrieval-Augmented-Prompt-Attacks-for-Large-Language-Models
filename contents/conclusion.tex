\section{Limitations}
Despite promising results, AGART has important limitations that inform future research directions. Computational overhead for graph maintenance scales with knowledge base size, potentially limiting real-time applications with very large attack corpora. Attack effectiveness depends on the quality and diversity of historical attack data, making the framework less effective in entirely novel domains.

\section{Future Work}
Based on our findings and identified limitations, we propose three directions for future research. Advanced graph representations should be explored, including hierarchical graph structures and dynamic graph evolution mechanisms to better model complex attack relationships and adaptation patterns, helping overcome current scalability limitations while enabling more nuanced semantic relationships. Additionally, expanding to multimodal attacks represents an important frontier, extending the framework to handle combinations of text, images, audio, and other modalities, as understanding cross-modal attack transfer will be crucial for comprehensive security evaluation of increasingly prevalent multimodal LLMs. Finally, defensive applications of the framework should be investigated, as the same graph-based representation that enables effective attacks could be repurposed to develop more robust defense mechanisms, potentially enabling co-evolutionary approaches where defensive and offensive systems improve together.

\section{Conclusion}
This paper presents AGART, a novel adaptive framework for LLM security evaluation that leverages graph-based knowledge representation to model semantic relationships between attack strategies. By integrating GraphRAG with adaptive optimization techniques, our approach enables more effective, efficient, and interpretable red-teaming than existing methods. Our evaluation demonstrates AGART's ability to systematically identify model vulnerabilities through structured exploration of the attack space, providing security researchers with valuable insights for developing more robust safety mechanisms. As LLMs continue to be deployed in increasingly sensitive applications, frameworks like AGART that can adapt to evolving defenses will be essential for ensuring these systems remain aligned with human values and resistant to adversarial manipulation. The broader impact of this work extends beyond security testing, potentially informing new approaches to model alignment, interpretability, and robust deployment practices for the next generation of language models. 